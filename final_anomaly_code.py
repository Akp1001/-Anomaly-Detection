# -*- coding: utf-8 -*-
"""FINAL_ANOMALY_CODE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-MTemNtcYe9Qbd8pDQ94xXKu3OoLhWJ6
"""

# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vYzISWRXkILhyY-UXUyetrhs2GKWlQjI
"""

# merged_anomaly_pipeline_with_evaluation.py
# -*- coding: utf-8 -*-
"""
Merged Anomaly Detection Pipeline with Exploratory Plots and Evaluation
Includes: Autoencoder (MLPRegressor), RandomForest, XGBoost, optional GPT mock,
and plotting: distributions, boxplots, correlation heatmap, ROC and PR curves.

Usage:
    python merged_anomaly_pipeline_with_evaluation.py
Adjust train_csv and test_csv paths in the __main__ section.
"""
import os
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, precision_recall_curve, roc_curve, auc
from sklearn.covariance import EllipticEnvelope
from imblearn.over_sampling import SMOTE
import xgboost as xgb
from scipy import stats
from scipy.stats import friedmanchisquare, rankdata

try:
    import openai
except ImportError:
    openai = None

# -------------------------
# Utility plotting functions
# -------------------------
def plot_feature_distributions(df, features, out_path=None):
    n = len(features)
    if n == 0:
        return
    cols = 2
    rows = int(np.ceil(n / cols))
    plt.figure(figsize=(14, 6 * rows))
    for i, feat in enumerate(features, 1):
        plt.subplot(rows, cols, i)
        sns.histplot(df[feat].dropna(), bins=40, kde=False)
        plt.title(feat)
        plt.grid(True, linestyle='--', alpha=0.4)
    plt.suptitle("Feature Distributions", fontsize=16, y=0.92)
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    if out_path:
        plt.savefig(out_path, dpi=200)
    plt.show()


def plot_boxplots_by_target(df, pairs, out_path=None, figsize=(14, 10), suptitle=""):
    n = len(pairs)
    if n == 0:
        return
    cols = 2
    rows = int(np.ceil(n / cols))
    plt.figure(figsize=figsize)
    for i, (target_col, feat_col) in enumerate(pairs, 1):
        plt.subplot(rows, cols, i)
        sns.boxplot(x=target_col, y=feat_col, data=df)
        plt.title(f"{feat_col} vs {target_col}")
    if suptitle:
        plt.suptitle(suptitle, fontsize=14, y=0.92)
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    if out_path:
        plt.savefig(out_path, dpi=200)
    plt.show()


def plot_boxplots_grouped(df, feature_list, target='Unusual', out_path=None, figsize=(14, 10)):
    n = len(feature_list)
    if n == 0:
        return
    cols = 2
    rows = int(np.ceil(n / cols))
    plt.figure(figsize=figsize)
    for i, feat in enumerate(feature_list, 1):
        plt.subplot(rows, cols, i)
        sns.boxplot(x=target, y=feat, data=df, showfliers=True)
        plt.title(f"{feat} vs {target}")
    plt.tight_layout()
    if out_path:
        plt.savefig(out_path, dpi=200)
    plt.show()


def plot_correlation_heatmap(df, features, out_path=None, figsize=(12, 10)):
    if len(features) == 0:
        return
    corr = df[features].corr()
    plt.figure(figsize=figsize)
    sns.heatmap(corr, annot=False, cmap='coolwarm', vmin=-1, vmax=1,
                xticklabels=features, yticklabels=features, square=True, cbar_kws={'shrink': .8})
    plt.title("Feature Correlation Heatmap")
    plt.tight_layout()
    if out_path:
        plt.savefig(out_path, dpi=200)
    plt.show()


# -------------------------
# Evaluation Functions
# -------------------------
def evaluate_models(y_true, y_pred_dict):
    """
    y_pred_dict: dict[name] = numpy array of predicted labels (0/1)
    Returns dict of metrics per model.
    """
    results = {}
    for name, preds in y_pred_dict.items():
        precision = precision_score(y_true, preds, zero_division=0)
        recall = recall_score(y_true, preds, zero_division=0)
        f1 = f1_score(y_true, preds, zero_division=0)
        # If only labels are available, roc_auc_score will compute based on labels (ok but coarse).
        try:
            auc_score = roc_auc_score(y_true, preds)
        except Exception:
            auc_score = float('nan')

        results[name] = {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc_score
        }
    return results


def plot_roc_curves(y_true, y_pred_dict):
    plt.figure(figsize=(10, 8))
    for name, preds in y_pred_dict.items():
        try:
            fpr, tpr, _ = roc_curve(y_true, preds)
            auc_score = auc(fpr, tpr)
            plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')
        except Exception:
            continue
    plt.plot([0, 1], [0, 1], 'k--', label='Random')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves')
    plt.legend()
    plt.grid(True)
    plt.show()


def plot_precision_recall(y_true, y_pred_dict):
    plt.figure(figsize=(10, 8))
    for name, preds in y_pred_dict.items():
        try:
            precision, recall, _ = precision_recall_curve(y_true, preds)
            auc_score = auc(recall, precision)
            plt.plot(recall, precision, label=f'{name} (AUC = {auc_score:.3f})')
        except Exception:
            continue
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curves')
    plt.legend()
    plt.grid(True)
    plt.show()


# -------------------------
# Main detector class
# -------------------------
class MergedAnomalyDetectorGPT:
    def __init__(self, gpt_api_key=None):
        self.scaler = StandardScaler()
        self.cellname_mapping = None
        self.models = {}
        self.best_model = None
        self.best_model_name = None
        self.gpt_api_key = gpt_api_key
        if gpt_api_key and openai:
            openai.api_key = gpt_api_key

        # placeholders for datasets and features
        self.train_df = None
        self.test_df = None
        self.features = []
        self.X = self.X_scaled = self.X_test = self.X_test_scaled = None
        self.y = None
        self.X_train = self.X_val = self.y_train = self.y_val = None
        self.X_train_bal = self.y_train_bal = None

    def load_and_explore_data(self, train_path, test_path, show_plots=True, plots_dir="plots"):
        os.makedirs(plots_dir, exist_ok=True)
        print("Loading datasets...")
        # default delimiter ';' as in original script
        self.train_df = pd.read_csv(train_path, delimiter=';')
        self.test_df = pd.read_csv(test_path, delimiter=';')

        print("\n===== BASIC INFO (train) =====")
        print(self.train_df.info())
        print("\n===== MISSING VALUES (train) =====")
        print(self.train_df.isnull().sum())
        print("\n===== SUMMARY STATISTICS (train) =====")
        print(self.train_df.describe(include='all'))
        print("\n===== TARGET DISTRIBUTION =====")
        if 'Unusual' in self.train_df.columns:
            print(self.train_df['Unusual'].value_counts())
            print(f"Anomaly rate: {self.train_df['Unusual'].mean()*100:.2f}%")
        else:
            raise KeyError("Train file must contain 'Unusual' target column")

        # Primary exploratory plots
        if show_plots:
            candidate_features = [
                'PRBUsageUL', 'PRBUsageDL', 'meanThr_DL', 'meanThr_UL',
                'maxThr_DL', 'maxThr_UL', 'meanUE_DL', 'meanUE_UL',
                'maxUE_DL', 'maxUE_UL', 'maxUE_UL+DL'
            ]
            avail = [c for c in candidate_features if c in self.train_df.columns]
            plot_feature_distributions(self.train_df, avail[:6], out_path=os.path.join(plots_dir, "feature_distributions.png"))

            box_pairs = [('Unusual', f) for f in ['PRBUsageUL', 'PRBUsageDL', 'meanThr_DL', 'meanThr_UL'] if f in self.train_df.columns]
            plot_boxplots_by_target(self.train_df, box_pairs, out_path=os.path.join(plots_dir, "boxplots_main.png"),
                                    suptitle="Main features vs Unusual")

            extended_feats = [f for f in ['PRBUsageUL', 'PRBUsageDL', 'meanThr_DL', 'meanThr_UL', 'maxThr_DL', 'maxThr_UL'] if f in self.train_df.columns]
            plot_boxplots_grouped(self.train_df, extended_feats, out_path=os.path.join(plots_dir, "boxplots_extended.png"))

            features_for_corr = [f for f in [
                'PRBUsageUL', 'PRBUsageDL', 'meanThr_DL', 'meanThr_UL',
                'maxThr_DL', 'maxThr_UL', 'meanUE_DL', 'meanUE_UL',
                'maxUE_DL', 'maxUE_UL', 'maxUE_UL+DL'
            ] if f in self.train_df.columns]
            if features_for_corr:
                plot_correlation_heatmap(self.train_df, features_for_corr, out_path=os.path.join(plots_dir, "correlation_heatmap.png"))

    def _safe_label_encode_train(self, series):
        vals = series.astype(str).unique().tolist()
        mapping = {v: i for i, v in enumerate(vals)}
        self.cellname_mapping = mapping
        return series.astype(str).map(mapping).astype(int)

    def _safe_label_encode_test(self, series):
        if self.cellname_mapping is None:
            return series.astype('category').cat.codes
        else:
            return series.astype(str).map(lambda v: self.cellname_mapping.get(v, -1)).astype(int)

    def preprocess_data(self):
        print("Preprocessing data...")
        # Create Hour column if possible
        for df in [self.train_df, self.test_df]:
            if 'Time' in df.columns:
                try:
                    df['Hour'] = pd.to_datetime(df['Time'], format='%H:%M', errors='coerce').dt.hour.fillna(0).astype(int)
                except Exception:
                    df['Hour'] = 0
            else:
                df['Hour'] = 0

        # Safe encoding for CellName
        if 'CellName' in self.train_df.columns:
            self.train_df['CellName_encoded'] = self._safe_label_encode_train(self.train_df['CellName'])
            self.test_df['CellName_encoded'] = self._safe_label_encode_test(self.test_df['CellName'])
        else:
            self.train_df['CellName_encoded'] = 0
            self.test_df['CellName_encoded'] = 0

        # Add engineered features if columns exist
        for df in [self.train_df, self.test_df]:
            if 'PRBUsageUL' in df.columns and 'PRBUsageDL' in df.columns:
                df['Total_PRB_Usage'] = df['PRBUsageUL'].fillna(0) + df['PRBUsageDL'].fillna(0)
            else:
                df['Total_PRB_Usage'] = 0

            if 'meanThr_DL' in df.columns and 'meanThr_UL' in df.columns:
                df['Throughput_Ratio'] = df['meanThr_DL'].fillna(0) / (df['meanThr_UL'].fillna(0) + 1e-8)
            else:
                df['Throughput_Ratio'] = 0

        # feature list: include only columns that actually exist
        candidate_features = ['PRBUsageUL', 'PRBUsageDL', 'meanThr_DL', 'meanThr_UL',
                              'maxThr_DL', 'maxThr_UL', 'meanUE_DL', 'meanUE_UL',
                              'maxUE_DL', 'maxUE_UL', 'maxUE_UL+DL', 'Hour',
                              'CellName_encoded', 'Total_PRB_Usage', 'Throughput_Ratio']
        self.features = [f for f in candidate_features if f in self.train_df.columns]

        print(f"Using features: {self.features}")

        # Prepare X and y
        if len(self.features) == 0:
            raise RuntimeError("No candidate features found in training data. Check column names.")

        self.X = self.train_df[self.features].fillna(0).astype(float).values
        self.y = self.train_df['Unusual'].astype(int).values
        self.X_test = self.test_df[self.features].fillna(0).astype(float).values

        # Scale
        self.X_scaled = self.scaler.fit_transform(self.X)
        self.X_test_scaled = self.scaler.transform(self.X_test)

        # Train/val split with stratify
        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(
            self.X_scaled, self.y, test_size=0.2, random_state=42, stratify=self.y
        )

        print(f"Before SMOTE - Train set: {self.X_train.shape}, Class counts: {np.bincount(self.y_train)}")

        # Balance with SMOTE
        smote = SMOTE(random_state=42)
        self.X_train_bal, self.y_train_bal = smote.fit_resample(self.X_train, self.y_train)
        print(f"After SMOTE - Train set: {self.X_train_bal.shape}, Class counts: {np.bincount(self.y_train_bal)}")

    def train_autoencoder(self, epochs=50):
        print("Training Autoencoder (MLPRegressor) ...")
        # Train only on 'normal' data (label 0) from the balanced train set
        normal_mask = (self.y_train_bal == 0)
        if normal_mask.sum() < 5:
            print("Not enough normal samples to train AE; skipping autoencoder.")
            return

        normal_data = self.X_train_bal[normal_mask]
        mlp = MLPRegressor(hidden_layer_sizes=(32, 16, 32), activation='relu',
                           solver='adam', alpha=0.01, learning_rate_init=0.01,
                           max_iter=epochs, early_stopping=True,
                           validation_fraction=0.1, n_iter_no_change=10,
                           random_state=42)
        mlp.fit(normal_data, normal_data)

        # Predict reconstructions on train and val
        train_pred = mlp.predict(self.X_train)
        val_pred = mlp.predict(self.X_val)

        train_mse = np.mean((self.X_train - train_pred) ** 2, axis=1)
        val_mse = np.mean((self.X_val - val_pred) ** 2, axis=1)

        # Determine threshold from training normal samples (95th percentile)
        try:
            threshold = np.percentile(train_mse[self.y_train == 0], 95)
        except Exception:
            threshold = np.percentile(train_mse, 95)

        val_labels = (val_mse > threshold).astype(int)

        f1 = f1_score(self.y_val, val_labels, zero_division=0)
        prec = precision_score(self.y_val, val_labels, zero_division=0)
        rec = recall_score(self.y_val, val_labels, zero_division=0)

        print(f"Autoencoder - F1: {f1:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}")

        self.models['autoencoder'] = {'model': mlp, 'threshold': threshold,
                                      'f1_score': f1, 'precision': prec, 'recall': rec}

    def train_xgboost(self):
        print("Training XGBoost...")
        xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1,
                                      scale_pos_weight=1, random_state=42, n_jobs=-1,
                                      use_label_encoder=False, eval_metric='logloss')
        xgb_model.fit(self.X_train_bal, self.y_train_bal)
        val_pred = xgb_model.predict(self.X_val)

        f1 = f1_score(self.y_val, val_pred, zero_division=0)
        prec = precision_score(self.y_val, val_pred, zero_division=0)
        rec = recall_score(self.y_val, val_pred, zero_division=0)

        print(f"XGBoost - F1: {f1:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}")

        self.models['xgboost'] = {'model': xgb_model, 'f1_score': f1,
                                  'precision': prec, 'recall': rec}

    def train_random_forest(self):
        print("Training RandomForest...")
        rf = RandomForestClassifier(n_estimators=50, max_depth=8, min_samples_split=10,
                                    min_samples_leaf=5, class_weight='balanced',
                                    random_state=42, n_jobs=-1)
        rf.fit(self.X_train_bal, self.y_train_bal)
        val_pred = rf.predict(self.X_val)

        f1 = f1_score(self.y_val, val_pred, zero_division=0)
        prec = precision_score(self.y_val, val_pred, zero_division=0)
        rec = recall_score(self.y_val, val_pred, zero_division=0)

        print(f"RandomForest - F1: {f1:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}")

        self.models['random_forest'] = {'model': rf, 'f1_score': f1,
                                        'precision': prec, 'recall': rec}

    def train_gpt(self):
        """
        Mocked GPT detector. If API key provided and you wish to call GPT, implement appropriate prompt/usage.
        Here we just create random predictions as a placeholder.
        """
        print("GPT-based detector (mocked)...")
        # In an actual implementation you'd query the model with prompts derived from records.
        # For now we randomly guess on the validation set to produce evaluation numbers.
        gpt_preds = np.random.choice([0, 1], size=len(self.X_val))
        f1 = f1_score(self.y_val, gpt_preds, zero_division=0)
        prec = precision_score(self.y_val, gpt_preds, zero_division=0)
        rec = recall_score(self.y_val, gpt_preds, zero_division=0)

        print(f"GPT Detector (mock) - F1: {f1:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}")

        self.models['gpt'] = {'model': None, 'f1_score': f1,
                              'precision': prec, 'recall': rec}

    def select_best_model(self):
        print("\n" + "="*60)
        print("MODEL COMPARISON RESULTS")
        print("="*60)

        # Collect all metrics for visualization
        model_names = []
        f1_scores = []
        precision_scores = []
        recall_scores = []
        weighted_scores = []

        best_name, best_score = None, -1

        # Create detailed metrics table
        print(f"{'Model':<20} {'F1 Score':<10} {'Precision':<12} {'Recall':<10} {'Weighted':<10}")
        print("-" * 70)

        for name, info in self.models.items():
            f1 = info.get('f1_score', 0)
            precision = info.get('precision', 0)
            recall = info.get('recall', 0)
            score = 0.4 * precision + 0.3 * f1 + 0.3 * recall

            # Store for visualization
            model_names.append(name.replace('_', ' ').title())
            f1_scores.append(f1)
            precision_scores.append(precision)
            recall_scores.append(recall)
            weighted_scores.append(score)

            # Print formatted row
            print(f"{name.replace('_', ' ').title():<20} {f1:<10.4f} {precision:<12.4f} {recall:<10.4f} {score:<10.4f}")

            if score > best_score:
                best_score, best_name = score, name

        print("-" * 70)

        if best_name is None:
            raise RuntimeError("No models trained or evaluated â€” cannot select best model.")

        # Create visualization plots
        self._plot_model_comparison(model_names, f1_scores, precision_scores, recall_scores, weighted_scores)

        # Create radar chart for comprehensive comparison
        self._plot_radar_chart(model_names, f1_scores, precision_scores, recall_scores)

        self.best_model_name = best_name
        self.best_model = self.models[best_name]

        print(f"\nðŸ† BEST MODEL: {best_name.replace('_', ' ').title().upper()}")
        print(f"   Weighted Score: {best_score:.4f}")
        print(f"   F1 Score: {self.best_model.get('f1_score', 0):.4f}")
        print(f"   Precision: {self.best_model.get('precision', 0):.4f}")
        print(f"   Recall: {self.best_model.get('recall', 0):.4f}")
        print("="*60)

    def _plot_model_comparison(self, model_names, f1_scores, precision_scores, recall_scores, weighted_scores):
        """Create bar charts comparing all models across different metrics"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # F1 Score comparison
        bars1 = ax1.bar(model_names, f1_scores, color='skyblue', alpha=0.8)
        ax1.set_title('F1 Score Comparison', fontsize=14, fontweight='bold')
        ax1.set_ylabel('F1 Score')
        ax1.set_ylim(0, 1)
        ax1.grid(axis='y', alpha=0.3)
        for i, bar in enumerate(bars1):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom', fontweight='bold')

        # Precision comparison
        bars2 = ax2.bar(model_names, precision_scores, color='lightgreen', alpha=0.8)
        ax2.set_title('Precision Comparison', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Precision')
        ax2.set_ylim(0, 1)
        ax2.grid(axis='y', alpha=0.3)
        for i, bar in enumerate(bars2):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom', fontweight='bold')

        # Recall comparison
        bars3 = ax3.bar(model_names, recall_scores, color='salmon', alpha=0.8)
        ax3.set_title('Recall Comparison', fontsize=14, fontweight='bold')
        ax3.set_ylabel('Recall')
        ax3.set_ylim(0, 1)
        ax3.grid(axis='y', alpha=0.3)
        for i, bar in enumerate(bars3):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom', fontweight='bold')

        # Weighted Score comparison
        bars4 = ax4.bar(model_names, weighted_scores, color='gold', alpha=0.8)
        ax4.set_title('Weighted Score Comparison', fontsize=14, fontweight='bold')
        ax4.set_ylabel('Weighted Score')
        ax4.set_ylim(0, 1)
        ax4.grid(axis='y', alpha=0.3)
        for i, bar in enumerate(bars4):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom', fontweight='bold')

        # Rotate x-axis labels for better readability
        for ax in [ax1, ax2, ax3, ax4]:
            ax.tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold', y=1.02)
        plt.show()

    def _plot_radar_chart(self, model_names, f1_scores, precision_scores, recall_scores):
        """Create radar chart for comprehensive model comparison"""
        fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))

        # Define the metrics
        metrics = ['F1 Score', 'Precision', 'Recall']
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]  # Complete the circle

        # Colors for different models
        colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']

        for i, model in enumerate(model_names):
            values = [f1_scores[i], precision_scores[i], recall_scores[i]]
            values += values[:1]  # Complete the circle

            ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i % len(colors)])
            ax.fill(angles, values, alpha=0.1, color=colors[i % len(colors)])

        # Customize the chart
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics, fontsize=12)
        ax.set_ylim(0, 1)
        ax.set_title('Model Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax.grid(True)

        plt.tight_layout()
        plt.show()

    def _plot_combined_metrics(self):
        """Create a combined metrics visualization"""
        if not self.models:
            return

        fig, ax = plt.subplots(figsize=(12, 6))

        model_names = [name.replace('_', ' ').title() for name in self.models.keys()]
        x = np.arange(len(model_names))
        width = 0.25

        f1_scores = [info.get('f1_score', 0) for info in self.models.values()]
        precision_scores = [info.get('precision', 0) for info in self.models.values()]
        recall_scores = [info.get('recall', 0) for info in self.models.values()]

        bars1 = ax.bar(x - width, f1_scores, width, label='F1 Score', color='skyblue', alpha=0.8)
        bars2 = ax.bar(x, precision_scores, width, label='Precision', color='lightgreen', alpha=0.8)
        bars3 = ax.bar(x + width, recall_scores, width, label='Recall', color='salmon', alpha=0.8)

        ax.set_xlabel('Models', fontweight='bold')
        ax.set_ylabel('Score', fontweight='bold')
        ax.set_title('Model Performance Metrics Comparison', fontsize=14, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(model_names, rotation=45, ha='right')
        ax.legend()
        ax.grid(axis='y', alpha=0.3)
        ax.set_ylim(0, 1)

        # Add value labels on bars
        for bars in [bars1, bars2, bars3]:
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{height:.3f}', ha='center', va='bottom', fontsize=9)

        plt.tight_layout()
        plt.show()

    def generate_predictions(self, output_csv='anomaly_predictions.csv'):
        if self.best_model_name is None:
            raise RuntimeError("Best model not selected. Call select_best_model() first.")

        print(f"Generating predictions using {self.best_model_name}...")
        model_info = self.best_model
        model = model_info.get('model', None)

        if self.best_model_name == 'autoencoder' and model is not None:
            pred = model.predict(self.X_test_scaled)
            mse = np.mean((self.X_test_scaled - pred) ** 2, axis=1)
            predictions = (mse > model_info['threshold']).astype(int)
        elif self.best_model_name == 'xgboost' and model is not None:
            predictions = model.predict(self.X_test_scaled)
        elif self.best_model_name == 'random_forest' and model is not None:
            predictions = model.predict(self.X_test_scaled)
        elif self.best_model_name == 'gpt':
            predictions = np.random.choice([0, 1], size=len(self.X_test_scaled))
        else:
            predictions = np.zeros(len(self.X_test_scaled), dtype=int)

        out_df = pd.DataFrame({'ID': range(len(predictions)), 'Unusual': predictions})
        out_df.to_csv(output_csv, index=False)
        print(f"Saved predictions to '{output_csv}'")
        print(f"Anomalies: {predictions.sum()} of {len(predictions)} ({predictions.mean()*100:.2f}%)")

    def evaluate_all_models(self):
        """
        Evaluate all trained models on validation set and plot ROC/PR curves.
        Returns the dict of results.
        """
        if self.models is None or len(self.models) == 0:
            raise RuntimeError("No models available to evaluate. Train models first.")

        y_preds = {}
        for name, info in self.models.items():
            model = info.get('model', None)
            if name == 'autoencoder' and model is not None:
                val_pred = model.predict(self.X_val)
                val_mse = np.mean((self.X_val - val_pred) ** 2, axis=1)
                preds = (val_mse > info.get('threshold', np.percentile(val_mse, 95))).astype(int)
            elif name in ['xgboost', 'random_forest'] and model is not None:
                preds = model.predict(self.X_val)
            elif name == 'gpt':
                preds = np.random.choice([0, 1], size=len(self.X_val))
            else:
                preds = np.zeros(len(self.X_val), dtype=int)
            y_preds[name] = preds

        results = evaluate_models(self.y_val, y_preds)
        print("\nMODEL EVALUATION RESULTS (on validation set):")
        for name, metrics in results.items():
            print(f"{name}: {metrics}")

        # Plot ROC and PR curves
        plot_roc_curves(self.y_val, y_preds)
        plot_precision_recall(self.y_val, y_preds)

        return results

    def run_pipeline(self, train_path, test_path, epochs=50, show_plots=True, plots_dir="plots", output_csv='anomaly_predictions.csv'):
        """
        High-level runner: load/explore -> preprocess -> train models -> select best -> predict -> evaluate
        """
        self.load_and_explore_data(train_path, test_path, show_plots=show_plots, plots_dir=plots_dir)
        self.preprocess_data()
        # Train models
        self.train_autoencoder(epochs=epochs)
        self.train_xgboost()
        self.train_random_forest()
        self.train_gpt()
        # Select best model and produce predictions
        self.select_best_model()
        self.generate_predictions(output_csv=output_csv)
        # Evaluate all models on validation set (plots + metrics)
        eval_results = self.evaluate_all_models()
        return eval_results


# -------------------------
# Run the pipeline (main)
# -------------------------
def main():
    # Customize these to your CSV file locations
    train_csv = 'ML-MATT-CompetitionQT2021_train.csv'
    test_csv = 'ML-MATT-CompetitionQT2021_test.csv'
    plots_dir = "plots"
    output_csv = "anomaly_predictions.csv"
    epochs = 50

    detector = MergedAnomalyDetectorGPT(gpt_api_key=None)
    results = detector.run_pipeline(train_path=train_csv, test_path=test_csv,
                                    epochs=epochs, show_plots=True, plots_dir=plots_dir,
                                    output_csv=output_csv)
    print("\nPipeline finished. Evaluation results returned.")
    return results


if __name__ == "__main__":
    main()

